{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Default Risk Modeling\n",
    "### End-to-End Credit Risk Pipeline: From Raw Data to Business Decisions\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Risk Data Science Team  \n",
    "**Dataset:** Synthetic consumer loan portfolio (50,000 loans)  \n",
    "**Objective:** Build a calibrated probability-of-default model and use it to optimise loan approval decisions\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Every time a bank or fintech company receives a loan application, it faces the same fundamental question: **will this borrower repay?** Getting this wrong in either direction is costly — approve too many risky borrowers and you suffer credit losses; reject too many creditworthy applicants and you leave revenue on the table.\n",
    "\n",
    "This notebook walks through a complete, production-style credit risk modeling workflow:\n",
    "\n",
    "1. **Exploratory Data Analysis** — understand the portfolio, identify patterns\n",
    "2. **Feature Engineering** — transform raw data into model-ready signals\n",
    "3. **Imbalance Handling** — address the fact that defaults are rare events (~20%)\n",
    "4. **Model Training** — Logistic Regression baseline vs. Gradient Boosting\n",
    "5. **Model Evaluation** — ROC-AUC, KS statistic, Gini coefficient, Precision-Recall\n",
    "6. **Probability Calibration** — ensure predicted probabilities are reliable\n",
    "7. **Business Simulation** — find the profit-maximising approval threshold\n",
    "8. **SHAP Interpretability** — explain model decisions for compliance and trust\n",
    "9. **Conclusions** — model card, limitations, next steps\n",
    "\n",
    "The emphasis throughout is on **business relevance**: every technical choice is motivated by its impact on lending decisions, portfolio economics, and regulatory compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0 — Environment Setup\n",
    "\n",
    "We add the project root to Python's path so the `src/` modules can be imported directly. All random seeds are fixed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path so 'src' is importable\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Project modules\n",
    "from src.data_generation import generate_loan_dataset, get_feature_metadata\n",
    "from src.feature_engineering import engineer_features, get_feature_columns\n",
    "from src.modeling import (\n",
    "    apply_smote, train_logistic_regression,\n",
    "    train_gradient_boosting, calibrate_model,\n",
    "    save_model, load_model, cross_validate_model\n",
    ")\n",
    "from src.evaluation import (\n",
    "    compute_roc_auc, compute_ks_statistic, compute_gini_coefficient,\n",
    "    compute_brier_score, build_metrics_summary_table,\n",
    "    plot_roc_curves, plot_precision_recall_curves,\n",
    "    plot_ks_curve, plot_calibration_curve\n",
    ")\n",
    "from src.business_simulation import (\n",
    "    compute_expected_profit_per_loan, threshold_sweep,\n",
    "    find_optimal_threshold, compute_portfolio_summary,\n",
    "    plot_threshold_vs_approval_rate, plot_threshold_vs_profit\n",
    ")\n",
    "from src.visualization import (\n",
    "    set_portfolio_style, plot_class_distribution,\n",
    "    plot_feature_distributions, plot_correlation_heatmap,\n",
    "    plot_smote_comparison, plot_shap_summary,\n",
    "    plot_shap_waterfall, plot_shap_dependence,\n",
    "    plot_feature_importance_bar\n",
    ")\n",
    "\n",
    "# Global settings\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "set_portfolio_style()\n",
    "\n",
    "# Output paths\n",
    "FIG_DIR   = os.path.join(PROJECT_ROOT, 'reports', 'figures')\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "DATA_DIR  = os.path.join(PROJECT_ROOT, 'data')\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print('Environment ready.')\n",
    "print(f'Python:     {sys.version.split()[0]}')\n",
    "print(f'NumPy:      {np.__version__}')\n",
    "print(f'Pandas:     {pd.__version__}')\n",
    "print(f'Project:    {PROJECT_ROOT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 — Data Generation & Exploratory Data Analysis\n",
    "\n",
    "### The Dataset — Simulating a Consumer Loan Portfolio\n",
    "\n",
    "We work with a synthetic consumer loan dataset designed to mirror the characteristics of a real sub-prime / near-prime lending book. The data is generated using a **latent risk score model**: each borrower has an unobserved creditworthiness score that determines their probability of default, and the observed features are correlated with that latent score in economically sensible ways.\n",
    "\n",
    "**Key features:**\n",
    "\n",
    "| Feature | Description |\n",
    "|---|---|\n",
    "| `credit_score` | FICO-style score (300–850). The single most important predictor of default. |\n",
    "| `credit_utilization` | Revolving credit usage ratio (0–1). High utilization signals financial stress. |\n",
    "| `debt_to_income` | Monthly debt / monthly income. A standard underwriting gate variable. |\n",
    "| `num_delinquencies` | Past delinquencies. Most predictive non-score credit bureau variable. |\n",
    "| `loan_amount` | Size of the loan request. Larger loans = larger potential losses. |\n",
    "| `interest_rate` | Annual rate, priced inversely to credit score. Correlated with risk. |\n",
    "| `loan_term` | 36 or 60 months. Longer terms have higher default rates. |\n",
    "| `annual_income` | Gross annual income. Log-normal distributed, right-skewed. |\n",
    "| `employment_length` | Years at employer. Proxy for income stability. |\n",
    "| `months_since_last_delinq` | Recency of delinquency. 40% missing = no delinquency history. |\n",
    "\n",
    "**Target:** `default` — 1 if the loan charged off within 2 years, 0 if current or fully paid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "print('Generating synthetic loan dataset...')\n",
    "df = generate_loan_dataset(n=50_000, default_rate=0.20, random_state=RANDOM_STATE)\n",
    "\n",
    "# Save raw data\n",
    "raw_path = os.path.join(DATA_DIR, 'raw', 'loans_raw.csv')\n",
    "df.to_csv(raw_path, index=False)\n",
    "print(f'Dataset saved to {raw_path}')\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Default rate: {df[\"default\"].mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "info_df = pd.DataFrame({\n",
    "    'dtype': df.dtypes,\n",
    "    'non_null': df.notna().sum(),\n",
    "    'null_count': df.isna().sum(),\n",
    "    'null_pct': (df.isna().sum() / len(df) * 100).round(1),\n",
    "    'unique': df.nunique(),\n",
    "})\n",
    "print('Dataset Info:')\n",
    "print(info_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numeric columns\n",
    "df.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_class_distribution(\n",
    "    df['default'],\n",
    "    ax=ax,\n",
    "    save_path=os.path.join(FIG_DIR, 'class_distribution.png')\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Class counts:\\n{df['default'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by default status\n",
    "numeric_features_to_plot = [\n",
    "    'credit_score', 'annual_income', 'credit_utilization',\n",
    "    'debt_to_income', 'loan_amount', 'interest_rate',\n",
    "    'num_delinquencies', 'num_credit_lines'\n",
    "]\n",
    "fig = plot_feature_distributions(\n",
    "    df,\n",
    "    features=numeric_features_to_plot,\n",
    "    hue_col='default',\n",
    "    n_cols=4,\n",
    "    save_path=os.path.join(FIG_DIR, 'feature_distributions.png')\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "numeric_df = df.select_dtypes(include=[np.number]).drop(columns=['loan_id'], errors='ignore')\n",
    "fig = plot_correlation_heatmap(\n",
    "    numeric_df,\n",
    "    figsize=(12, 9),\n",
    "    save_path=os.path.join(FIG_DIR, 'correlation_heatmap.png')\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default rate by key categorical segments\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, col in zip(axes, ['home_ownership', 'loan_purpose', 'loan_term']):\n",
    "    default_rate = df.groupby(col)['default'].mean().sort_values(ascending=False)\n",
    "    counts = df[col].value_counts()\n",
    "    \n",
    "    bars = ax.bar(\n",
    "        [str(x) for x in default_rate.index],\n",
    "        default_rate.values * 100,\n",
    "        color='#2196F3',\n",
    "        edgecolor='none'\n",
    "    )\n",
    "    ax.axhline(y=df['default'].mean() * 100, color='red', linestyle='--', lw=1.5,\n",
    "               label=f'Overall: {df[\"default\"].mean():.1%}')\n",
    "    \n",
    "    for bar, idx in zip(bars, default_rate.index):\n",
    "        n = counts.get(idx, 0)\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "                f'n={n:,}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax.set_title(f'Default Rate by {col.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax.set_ylabel('Default Rate (%)')\n",
    "    ax.legend(fontsize=9)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=30, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'default_rate_by_segment.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Key Findings\n",
    "\n",
    "1. **Class imbalance:** 80% performing vs. 20% default — a 4:1 ratio. This is realistic for a near-prime lending portfolio but will bias a naive classifier toward always predicting \"performing.\" We address this in Section 3.\n",
    "\n",
    "2. **Credit score is the primary separator:** The KDE distributions for defaulters vs. non-defaulters have clear separation on credit score, confirming its role as the dominant feature.\n",
    "\n",
    "3. **Interest rate and credit score are negatively correlated (~-0.75)** — this is by design (higher-risk borrowers pay higher rates), but it means including both features adds partial redundancy. We keep both because they carry different information (rate is a forward-looking pricing decision; score is a backward-looking assessment).\n",
    "\n",
    "4. **Small-business loans have the highest default rate** — followed by medical loans. Debt consolidation, while the most common purpose, is close to the portfolio average.\n",
    "\n",
    "5. **60-month loans default more than 36-month** — consistent with longer term = longer exposure window = more opportunities for financial stress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 — Feature Engineering\n",
    "\n",
    "### Transforming Raw Data Into Model-Ready Signals\n",
    "\n",
    "Raw features from a loan application are rarely in the ideal form for a statistical model. Feature engineering bridges that gap by:\n",
    "\n",
    "- **Reducing skewness** in income and loan amount variables (log transforms)\n",
    "- **Creating interactions** that capture combined effects not visible in individual features\n",
    "- **Encoding categories** in a way that preserves meaningful structure\n",
    "- **Handling missing data** without discarding information\n",
    "\n",
    "A key design requirement is **no data leakage**: all transformations that involve learning from data (bin edges, encodings) must be fitted on the training set only and then applied to the test set. Our `engineer_features(fit=True/False)` API enforces this.\n",
    "\n",
    "### Engineered Features\n",
    "\n",
    "| Feature | Type | Rationale |\n",
    "|---|---|---|\n",
    "| `log_annual_income` | Log transform | Income is right-skewed; log makes it approximately normal |\n",
    "| `log_loan_amount` | Log transform | Same reasoning |\n",
    "| `loan_to_income_ratio` | Interaction | Captures loan affordability relative to income |\n",
    "| `monthly_payment_est` | Derived | PMT formula — the actual monthly cash outflow |\n",
    "| `payment_to_income` | Interaction | Loan-specific debt service ratio (vs. aggregate DTI) |\n",
    "| `utilization_x_delinq` | Interaction | High utilization + past delinquencies = compounding risk |\n",
    "| `has_delinquency_history` | Flag | Missing `months_since_last_delinq` = no history = meaningful |\n",
    "| `credit_score_bin` | Risk tier | Ordinal bins: Poor / Fair / Good / Very Good / Exceptional |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split BEFORE feature engineering to prevent leakage\n",
    "# We stratify on 'default' to preserve the class ratio in both sets\n",
    "\n",
    "X_raw = df.drop(columns=['default'])\n",
    "y = df['default'].values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y,\n",
    "    test_size=0.20,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Reset indices\n",
    "X_train_raw = X_train_raw.reset_index(drop=True)\n",
    "X_test_raw  = X_test_raw.reset_index(drop=True)\n",
    "\n",
    "print(f'Training set:  {X_train_raw.shape[0]:,} loans ({y_train.mean():.1%} defaults)')\n",
    "print(f'Test set:      {X_test_raw.shape[0]:,} loans ({y_test.mean():.1%} defaults)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering — fit on train, apply to test\n",
    "# Add target back temporarily for engineer_features (it will be dropped internally)\n",
    "train_with_target = X_train_raw.copy()\n",
    "train_with_target['default'] = y_train\n",
    "test_with_target = X_test_raw.copy()\n",
    "test_with_target['default'] = y_test\n",
    "\n",
    "X_train_fe, encoder_state = engineer_features(train_with_target, fit=True)\n",
    "X_test_fe,  _             = engineer_features(test_with_target,  fit=False, encoder_state=encoder_state)\n",
    "\n",
    "print(f'Features after engineering: {X_train_fe.shape[1]}')\n",
    "print(f'Training set shape: {X_train_fe.shape}')\n",
    "print(f'Test set shape:     {X_test_fe.shape}')\n",
    "\n",
    "# Save processed data\n",
    "proc_path = os.path.join(DATA_DIR, 'processed', 'loans_processed.csv')\n",
    "train_processed = X_train_fe.copy()\n",
    "train_processed['default'] = y_train\n",
    "train_processed.to_csv(proc_path, index=False)\n",
    "print(f'Processed data saved to {proc_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the engineered feature set\n",
    "print('Engineered feature columns:')\n",
    "for i, col in enumerate(X_train_fe.columns, 1):\n",
    "    print(f'  {i:2d}. {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays for sklearn\n",
    "feature_names = X_train_fe.columns.tolist()\n",
    "X_train_np = X_train_fe.values.astype(np.float32)\n",
    "X_test_np  = X_test_fe.values.astype(np.float32)\n",
    "\n",
    "print(f'X_train: {X_train_np.shape}, dtype={X_train_np.dtype}')\n",
    "print(f'X_test:  {X_test_np.shape}, dtype={X_test_np.dtype}')\n",
    "print(f'Any NaN in train: {np.isnan(X_train_np).any()}')\n",
    "print(f'Any NaN in test:  {np.isnan(X_test_np).any()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 — Handling Class Imbalance\n",
    "\n",
    "### Why Accuracy Misleads in Credit Risk\n",
    "\n",
    "With a 20% default rate, a model that **always predicts \"performing\"** achieves 80% accuracy — but it catches zero defaults. In lending, this is catastrophic: every missed default represents a loss of principal plus accrued costs.\n",
    "\n",
    "We use two complementary strategies:\n",
    "\n",
    "**Strategy 1 — SMOTE (Synthetic Minority Oversampling)**  \n",
    "Creates synthetic default examples in the training set by interpolating between real default observations. Applied *after* the train/test split and *only to the training set* to avoid leakage. We use `sampling_strategy=0.5` (1:2 minority:majority) rather than full balancing to avoid introducing too many synthetic samples.\n",
    "\n",
    "**Strategy 2 — Class Weights (`class_weight='balanced'`)**  \n",
    "For Logistic Regression, we use sklearn's built-in class weighting as an alternative to SMOTE. The model loss function upweights the minority class proportionally to its underrepresentation. This is computationally cheaper but requires the model to support the parameter.\n",
    "\n",
    "> **Data leakage warning:** If SMOTE were applied to the full dataset before splitting, synthetic default samples would be generated from both the training *and* test set distributions. This would inflate test-set performance and produce optimistic AUC/KS estimates. Always SMOTE after splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to training set only\n",
    "print('Applying SMOTE to training set...')\n",
    "print(f'Before SMOTE — Class 0: {(y_train==0).sum():,}, Class 1: {(y_train==1).sum():,}')\n",
    "\n",
    "X_train_smote, y_train_smote = apply_smote(\n",
    "    X_train_np, y_train,\n",
    "    sampling_strategy=0.5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f'After  SMOTE — Class 0: {(y_train_smote==0).sum():,}, Class 1: {(y_train_smote==1).sum():,}')\n",
    "print(f'New default rate in training set: {y_train_smote.mean():.1%}')\n",
    "print(f'Training set size change: {len(y_train):,} → {len(y_train_smote):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the effect of SMOTE\n",
    "fig = plot_smote_comparison(\n",
    "    y_before=y_train,\n",
    "    y_after=y_train_smote,\n",
    "    save_path=os.path.join(FIG_DIR, 'smote_comparison.png')\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 — Model Training\n",
    "\n",
    "### Logistic Regression Baseline vs. Gradient Boosting\n",
    "\n",
    "We train two models that represent opposite ends of the interpretability-performance spectrum:\n",
    "\n",
    "**Logistic Regression**\n",
    "- The industry's traditional scorecarding baseline\n",
    "- Highly interpretable: coefficients directly show feature impact\n",
    "- Assumes linear relationship between features and log-odds of default\n",
    "- Regularised (L2) to prevent overfitting on correlated features\n",
    "- Features are scaled inside a Pipeline to prevent leakage\n",
    "\n",
    "**Gradient Boosting**\n",
    "- An ensemble of sequential decision trees, each correcting prior errors\n",
    "- Captures non-linear relationships and feature interactions automatically\n",
    "- Typically 5–10% higher AUC than logistic regression on structured data\n",
    "- Less interpretable natively — requires SHAP for explanation\n",
    "- Hyperparameters tuned conservatively to avoid overfitting\n",
    "\n",
    "**Probability Calibration**\n",
    "After training, the Gradient Boosting model is calibrated using Platt scaling (logistic regression on the raw model outputs). This maps the raw scores to well-calibrated probabilities — essential for using predicted PD in profit calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split SMOTE'd training data further: train (80%) and calibration (20%)\n",
    "# The calibration set is used to fit Platt scaling without touching the test set\n",
    "X_tr, X_cal, y_tr, y_cal = train_test_split(\n",
    "    X_train_smote, y_train_smote,\n",
    "    test_size=0.20,\n",
    "    stratify=y_train_smote,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f'Model training set:  {X_tr.shape[0]:,} samples')\n",
    "print(f'Calibration set:     {X_cal.shape[0]:,} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print('Training Logistic Regression...')\n",
    "lr_model = train_logistic_regression(\n",
    "    X_tr, y_tr,\n",
    "    C=0.1,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "save_model(lr_model, os.path.join(MODEL_DIR, 'logistic_regression.pkl'))\n",
    "print('  Done. Saved to models/logistic_regression.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting\n",
    "print('Training Gradient Boosting (this may take ~2-3 minutes)...')\n",
    "gb_model = train_gradient_boosting(\n",
    "    X_tr, y_tr,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "save_model(gb_model, os.path.join(MODEL_DIR, 'gradient_boosting.pkl'))\n",
    "print('  Done. Saved to models/gradient_boosting.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate Gradient Boosting with Platt scaling\n",
    "print('Calibrating Gradient Boosting (Platt scaling)...')\n",
    "gb_cal_model = calibrate_model(gb_model, X_cal, y_cal, method='sigmoid')\n",
    "save_model(gb_cal_model, os.path.join(MODEL_DIR, 'gradient_boosting_calibrated.pkl'))\n",
    "print('  Done. Saved to models/gradient_boosting_calibrated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set (held-out, never seen during training/calibration)\n",
    "lr_probs  = lr_model.predict_proba(X_test_np)[:, 1]\n",
    "gb_probs  = gb_model.predict_proba(X_test_np)[:, 1]\n",
    "gb_c_probs = gb_cal_model.predict_proba(X_test_np)[:, 1]\n",
    "\n",
    "print('Predicted probability ranges:')\n",
    "print(f'  Logistic Regression:           [{lr_probs.min():.3f}, {lr_probs.max():.3f}]')\n",
    "print(f'  Gradient Boosting (raw):       [{gb_probs.min():.3f}, {gb_probs.max():.3f}]')\n",
    "print(f'  Gradient Boosting (calibrated):[{gb_c_probs.min():.3f}, {gb_c_probs.max():.3f}]')\n",
    "\n",
    "print(f'\\nMean predicted PD on test set (true rate: {y_test.mean():.3f}):')\n",
    "print(f'  Logistic Regression:           {lr_probs.mean():.3f}')\n",
    "print(f'  Gradient Boosting (raw):       {gb_probs.mean():.3f}')\n",
    "print(f'  Gradient Boosting (calibrated):{gb_c_probs.mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for Logistic Regression (to get unbiased performance estimate)\n",
    "# Note: We cross-validate on original (non-SMOTE) training data\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLR\n",
    "\n",
    "lr_cv_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', SklearnLR(C=0.1, penalty='l2', solver='lbfgs',\n",
    "                      class_weight='balanced', max_iter=2000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "print('Cross-validating Logistic Regression (5-fold stratified)...')\n",
    "lr_cv_df = cross_validate_model(lr_cv_model, X_train_np, y_train, cv=5, random_state=RANDOM_STATE)\n",
    "print('\\nLogistic Regression — CV Results:')\n",
    "print(lr_cv_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 — Model Evaluation\n",
    "\n",
    "### Beyond Accuracy: Metrics for Credit Risk\n",
    "\n",
    "Credit risk models are evaluated on metrics that reflect their ability to **rank borrowers by risk** and **identify defaults at useful operating points**.\n",
    "\n",
    "**ROC-AUC (Area Under the ROC Curve)**  \n",
    "Measures overall discrimination ability. Interpretation: if we pick one random defaulter and one random non-defaulter, the AUC is the probability that the model scores the defaulter higher. An AUC of 0.83 means the model correctly ranks the defaulter higher 83% of the time. The ROC curve is robust to class imbalance.\n",
    "\n",
    "**KS Statistic (Kolmogorov-Smirnov)**  \n",
    "The maximum separation between the cumulative score distributions of defaulters and non-defaulters. Industry rule of thumb: KS > 0.40 is a good model, KS > 0.55 is excellent. Used as the primary model selection metric in many retail banking validation frameworks.\n",
    "\n",
    "**Gini Coefficient**  \n",
    "Gini = 2 × AUC − 1. Required in Basel III/IV internal ratings-based (IRB) model documentation. A Gini of 0.66 means the model is 66% better than random at ranking borrowers by credit risk.\n",
    "\n",
    "**Precision-Recall Curve**  \n",
    "Especially informative when the positive class (defaults) is rare and false positives are costly. Average Precision summarises the curve into a single number — higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics summary table\n",
    "models_results = {\n",
    "    'Logistic Regression':         {'y_true': y_test, 'y_prob': lr_probs},\n",
    "    'Gradient Boosting (raw)':     {'y_true': y_test, 'y_prob': gb_probs},\n",
    "    'Gradient Boosting (cal.)':    {'y_true': y_test, 'y_prob': gb_c_probs},\n",
    "}\n",
    "\n",
    "metrics_df = build_metrics_summary_table(models_results)\n",
    "print('Model Evaluation Summary:')\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "roc_models = {\n",
    "    'Logistic Regression':      (y_test, lr_probs),\n",
    "    'Gradient Boosting (raw)':  (y_test, gb_probs),\n",
    "    'Gradient Boosting (cal.)':(y_test, gb_c_probs),\n",
    "}\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_roc_curves(\n",
    "    roc_models,\n",
    "    ax=ax,\n",
    "    save_path=os.path.join(FIG_DIR, 'roc_curves.png')\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curves\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_precision_recall_curves(\n",
    "    roc_models,\n",
    "    baseline_positive_rate=y_test.mean(),\n",
    "    ax=ax,\n",
    "    save_path=os.path.join(FIG_DIR, 'precision_recall_curves.png')\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS chart for the best model (calibrated GBM)\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "plot_ks_curve(\n",
    "    y_test, gb_c_probs,\n",
    "    model_name='Gradient Boosting (Calibrated)',\n",
    "    ax=ax,\n",
    "    save_path=os.path.join(FIG_DIR, 'ks_chart.png')\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "ks_stat, ks_thresh = compute_ks_statistic(y_test, gb_c_probs)\n",
    "print(f'KS Statistic: {ks_stat:.4f}  at threshold: {ks_thresh:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report at default threshold (0.5)\n",
    "print('Classification Report — Gradient Boosting (Calibrated), threshold=0.5:')\n",
    "y_pred_gb = (gb_c_probs >= 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred_gb, target_names=['Performing', 'Default']))\n",
    "\n",
    "print('Note: threshold=0.5 is NOT the business-optimal threshold.')\n",
    "print('We will find the profit-optimal threshold in Section 7.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Summary\n",
    "\n",
    "The Gradient Boosting model significantly outperforms Logistic Regression on all discrimination metrics:\n",
    "- AUC improvement of ~7 percentage points (typical for tree ensembles on tabular credit data)\n",
    "- KS statistic improvement of ~11 points — at any given threshold, the GBM separates defaulters and non-defaulters more cleanly\n",
    "\n",
    "**Model selection:** The calibrated Gradient Boosting model is selected for the business simulation because:\n",
    "1. Higher discrimination (AUC, KS, Gini)\n",
    "2. Better calibrated probabilities (lower Brier score) — critical for profit calculations\n",
    "3. SHAP-compatible for regulatory explainability (Section 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 — Probability Calibration Diagnostics\n",
    "\n",
    "### Are the Predicted Probabilities Trustworthy?\n",
    "\n",
    "A model can have excellent discrimination (high AUC/KS) but produce poorly calibrated probabilities. Calibration measures whether the **magnitude** of predicted probabilities is realistic — if the model says a loan has a 30% chance of defaulting, does it actually default 30% of the time in practice?\n",
    "\n",
    "This matters enormously for credit risk because:\n",
    "- Expected profit calculations use the raw PD estimate directly\n",
    "- Regulatory stress testing requires the PD to represent a true through-the-cycle probability\n",
    "- Risk-based pricing (interest rate = f(PD)) requires calibrated PDs\n",
    "\n",
    "Tree ensembles are typically **overconfident** — they push predictions toward 0 and 1 more than warranted. The reliability diagram reveals this as a sigmoid curve deviating from the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliability diagram: uncalibrated vs. calibrated GBM\n",
    "cal_models = {\n",
    "    'GBM (uncalibrated)': (y_test, gb_probs),\n",
    "    'GBM (Platt scaled)': (y_test, gb_c_probs),\n",
    "    'Logistic Regression': (y_test, lr_probs),\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_calibration_curve(\n",
    "    cal_models,\n",
    "    n_bins=10,\n",
    "    ax=ax,\n",
    "    save_path=os.path.join(FIG_DIR, 'calibration_curve.png')\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brier score comparison\n",
    "brier_scores = {\n",
    "    'GBM (uncalibrated)':  compute_brier_score(y_test, gb_probs),\n",
    "    'GBM (Platt scaled)':  compute_brier_score(y_test, gb_c_probs),\n",
    "    'Logistic Regression': compute_brier_score(y_test, lr_probs),\n",
    "    'Naive baseline (always predict base rate)': y_test.mean() * (1 - y_test.mean()),\n",
    "}\n",
    "\n",
    "print('Brier Scores (lower = better, baseline for 20% default rate = 0.160):')\n",
    "for model, score in sorted(brier_scores.items(), key=lambda x: x[1]):\n",
    "    improvement = (brier_scores['Naive baseline (always predict base rate)'] - score)\n",
    "    improvement_pct = improvement / brier_scores['Naive baseline (always predict base rate)'] * 100\n",
    "    print(f'  {model:40s}: {score:.4f}  ({improvement_pct:+.1f}% vs. baseline)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Findings\n",
    "\n",
    "The reliability diagram confirms that:\n",
    "1. **The uncalibrated GBM** shows the classic overconfidence pattern — it pushes too many predictions toward 0 and 1, deviating from the diagonal at both extremes\n",
    "2. **Platt scaling** brings the GBM's curve much closer to the diagonal, reducing the Brier score and making the probabilities more trustworthy\n",
    "3. **Logistic Regression** is inherently better calibrated than tree models, though its discrimination is lower\n",
    "\n",
    "The calibrated GBM combines the best of both worlds: high discrimination from the tree ensemble, and reliable probability estimates from Platt scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 — Business Simulation: Threshold Optimisation\n",
    "\n",
    "### Setting the Approval Cutoff\n",
    "\n",
    "The most important decision a credit risk model informs is not \"is this borrower going to default?\" but rather **\"should we approve this loan application, and at what interest rate?\"**\n",
    "\n",
    "This requires choosing a **decision threshold**: the maximum predicted default probability we're willing to accept. Any applicant with predicted PD above the threshold is declined.\n",
    "\n",
    "The profit model per loan:\n",
    "\n",
    "```\n",
    "Expected Profit = (1 − PD) × Revenue_if_performing − PD × Loss_if_default\n",
    "\n",
    "where:\n",
    "  Revenue_if_performing = total_interest − origination_cost − funding_cost\n",
    "  Loss_if_default       = principal × (1 − recovery_rate) + origination_cost + funding_cost\n",
    "  \n",
    "Assumptions:\n",
    "  Recovery rate:    10% of principal (unsecured consumer lending)\n",
    "  Origination cost: $200 per loan\n",
    "  Funding cost:     4% annual cost of capital on avg outstanding balance\n",
    "```\n",
    "\n",
    "We sweep thresholds from 5% to 79% and compute total expected portfolio profit at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loan economics from the test set\n",
    "test_loan_amounts   = X_test_raw['loan_amount'].values\n",
    "test_interest_rates = X_test_raw['interest_rate'].values\n",
    "test_loan_terms     = X_test_raw['loan_term'].values\n",
    "\n",
    "print('Test set loan economics:')\n",
    "print(f'  Loans:           {len(test_loan_amounts):,}')\n",
    "print(f'  Avg loan amount: ${test_loan_amounts.mean():,.0f}')\n",
    "print(f'  Avg rate:        {test_interest_rates.mean():.1%}')\n",
    "print(f'  Total book:      ${test_loan_amounts.sum():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show per-loan profit for a single example\n",
    "example_profit = compute_expected_profit_per_loan(\n",
    "    loan_amount=15_000,\n",
    "    interest_rate=0.13,\n",
    "    loan_term_months=36,\n",
    "    default_probability=0.10\n",
    ")\n",
    "print(f'Example: $15,000 loan, 13% rate, 36 months, PD=10%')\n",
    "print(f'Expected profit: ${example_profit:,.2f}')\n",
    "\n",
    "example_loss = compute_expected_profit_per_loan(\n",
    "    loan_amount=15_000,\n",
    "    interest_rate=0.13,\n",
    "    loan_term_months=36,\n",
    "    default_probability=0.50\n",
    ")\n",
    "print(f'\\nSame loan at PD=50%:')\n",
    "print(f'Expected profit: ${example_loss:,.2f}  (expected loss)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run threshold sweep\n",
    "print('Running threshold sweep...')\n",
    "sweep_df = threshold_sweep(\n",
    "    y_true=y_test,\n",
    "    y_prob=gb_c_probs,\n",
    "    loan_amounts=test_loan_amounts,\n",
    "    interest_rates=test_interest_rates,\n",
    "    loan_terms=test_loan_terms,\n",
    ")\n",
    "print(f'Evaluated {len(sweep_df)} threshold values.')\n",
    "print('\\nSample rows from sweep:')\n",
    "print(sweep_df[['threshold','n_approved','approval_rate','default_rate_approved',\n",
    "                'total_expected_profit','avg_profit_per_loan']].iloc[::10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold\n",
    "opt_threshold, opt_row = find_optimal_threshold(sweep_df, objective='total_profit')\n",
    "\n",
    "print(f'Optimal approval threshold (profit-maximising): {opt_threshold:.2f}')\n",
    "print(f'  Approval rate:                {opt_row[\"approval_rate\"]:.1%}')\n",
    "print(f'  Default rate (approved pool): {opt_row[\"default_rate_approved\"]:.1%}')\n",
    "print(f'  Total expected profit:        ${opt_row[\"total_expected_profit\"]:,.0f}')\n",
    "print(f'  Avg profit per loan:          ${opt_row[\"avg_profit_per_loan\"]:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Volume vs. Credit Quality trade-off\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_threshold_vs_approval_rate(\n",
    "    sweep_df,\n",
    "    ax=ax,\n",
    "    save_path=os.path.join(FIG_DIR, 'threshold_approval_rate.png')\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Expected profit by threshold\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_threshold_vs_profit(\n",
    "    sweep_df,\n",
    "    optimal_threshold=opt_threshold,\n",
    "    ax=ax,\n",
    "    save_path=os.path.join(FIG_DIR, 'threshold_profit.png')\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio summary at optimal threshold\n",
    "portfolio = compute_portfolio_summary(\n",
    "    y_true=y_test,\n",
    "    y_prob=gb_c_probs,\n",
    "    loan_amounts=test_loan_amounts,\n",
    "    interest_rates=test_interest_rates,\n",
    "    loan_terms=test_loan_terms,\n",
    "    threshold=opt_threshold\n",
    ")\n",
    "\n",
    "print('='*55)\n",
    "print(' PORTFOLIO SUMMARY AT OPTIMAL THRESHOLD')\n",
    "print('='*55)\n",
    "print(f' Approval threshold:           PD ≤ {portfolio[\"approval_threshold\"]:.0%}')\n",
    "print(f' Applications evaluated:       {portfolio[\"n_evaluated\"]:,}')\n",
    "print(f' Loans approved:               {portfolio[\"n_approved\"]:,} ({portfolio[\"approval_rate\"]:.1%})')\n",
    "print(f' Loans declined:               {portfolio[\"n_declined\"]:,} ({portfolio[\"decline_rate\"]:.1%})')\n",
    "print(f'')\n",
    "print(f' Actual defaults in approved:  {portfolio[\"n_true_defaults_approved\"]:,}')\n",
    "print(f' Default rate (approved pool): {portfolio[\"default_rate_in_approved\"]:.1%}')\n",
    "print(f' (vs. population rate:         {y_test.mean():.1%})')\n",
    "print(f'')\n",
    "print(f' Total loan book value:        ${portfolio[\"total_loan_book_value\"]:>14,.0f}')\n",
    "print(f' Expected revenue (good loans):${portfolio[\"expected_revenue_from_good_loans\"]:>14,.0f}')\n",
    "print(f' Expected losses (defaults):   ${portfolio[\"expected_loss_from_defaults\"]:>14,.0f}')\n",
    "print(f' NET EXPECTED PROFIT:          ${portfolio[\"expected_total_profit\"]:>14,.0f}')\n",
    "print(f' Return on portfolio:          {portfolio[\"return_on_portfolio\"]:.2%}')\n",
    "print('='*55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare vs. naive strategies\n",
    "# Strategy 1: Approve everyone\n",
    "approve_all = compute_portfolio_summary(\n",
    "    y_test, gb_c_probs, test_loan_amounts, test_interest_rates, test_loan_terms,\n",
    "    threshold=1.0\n",
    ")\n",
    "\n",
    "# Strategy 2: Traditional credit score cutoff (FICO > 660)\n",
    "# Simulate by creating a 'dummy' probability that's low for high-score borrowers\n",
    "fico_scores = X_test_raw['credit_score'].values\n",
    "fico_dummy_pd = 1 - (fico_scores - 300) / (850 - 300)  # Inverse normalised score\n",
    "fico_threshold = float(fico_dummy_pd[fico_scores >= 660].max())  # threshold equivalent to FICO 660\n",
    "fico_strategy = compute_portfolio_summary(\n",
    "    y_test, fico_dummy_pd, test_loan_amounts, test_interest_rates, test_loan_terms,\n",
    "    threshold=fico_threshold\n",
    ")\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {'Strategy': 'Approve All',\n",
    "     'Approval Rate': f\"{approve_all['approval_rate']:.1%}\",\n",
    "     'Default Rate': f\"{approve_all['default_rate_in_approved']:.1%}\",\n",
    "     'Expected Profit': f\"${approve_all['expected_total_profit']:,.0f}\"},\n",
    "    {'Strategy': 'FICO > 660 cutoff',\n",
    "     'Approval Rate': f\"{fico_strategy['approval_rate']:.1%}\",\n",
    "     'Default Rate': f\"{fico_strategy['default_rate_in_approved']:.1%}\",\n",
    "     'Expected Profit': f\"${fico_strategy['expected_total_profit']:,.0f}\"},\n",
    "    {'Strategy': f'ML Model (threshold={opt_threshold:.2f})',\n",
    "     'Approval Rate': f\"{portfolio['approval_rate']:.1%}\",\n",
    "     'Default Rate': f\"{portfolio['default_rate_in_approved']:.1%}\",\n",
    "     'Expected Profit': f\"${portfolio['expected_total_profit']:,.0f}\"},\n",
    "])\n",
    "\n",
    "print('Strategy Comparison:')\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Simulation Findings\n",
    "\n",
    "The profit curve reveals a clear optimal point where expected portfolio profit is maximised. Key takeaways:\n",
    "\n",
    "1. **Approving everyone** is suboptimal — default losses overwhelm interest revenue in the high-risk tail of the population.\n",
    "\n",
    "2. **A single FICO cutoff** is a blunt instrument — it ignores DTI, utilization, delinquency history, and loan characteristics that add independent predictive power.\n",
    "\n",
    "3. **The ML model at its optimal threshold** delivers higher profit than either naive strategy by more precisely identifying which high-FICO borrowers still have elevated risk (high utilization, recent delinquencies) and which moderate-FICO borrowers are actually low risk (stable income, low DTI, long employment).\n",
    "\n",
    "4. The **default rate in the approved pool drops from 20% (population) to ~11%** at the optimal threshold — a 45% reduction in credit risk while maintaining ~70% approval rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8 — Model Interpretability with SHAP\n",
    "\n",
    "### Explainability for Regulatory Compliance\n",
    "\n",
    "In the United States, the **Equal Credit Opportunity Act (ECOA)** and **Fair Credit Reporting Act (FCRA)** require lenders to provide applicants with the specific reasons their application was declined (\"adverse action reasons\"). This makes black-box models a compliance liability.\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** provides theoretically grounded feature attribution values based on game theory. For each prediction, SHAP decomposes the model output into contributions from each feature:\n",
    "\n",
    "```\n",
    "Model Output = Base Value (avg prediction) + SHAP₁ + SHAP₂ + ... + SHAPₙ\n",
    "```\n",
    "\n",
    "A positive SHAP value means the feature pushed the prediction toward default (higher risk); negative means it pushed toward performing (lower risk).\n",
    "\n",
    "SHAP values satisfy three desirable properties:\n",
    "- **Consistency:** if a feature becomes more important, its SHAP value increases\n",
    "- **Local accuracy:** SHAP values sum exactly to the prediction\n",
    "- **Dummy feature:** features with no impact get zero SHAP value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Extract the underlying GBM from the calibrated model\n",
    "# CalibratedClassifierCV wraps the original model — we access it via .estimator\n",
    "base_gbm = gb_cal_model.estimator\n",
    "\n",
    "# Use a sample of 2000 test observations for SHAP (computing all 10k is slow)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "shap_sample_idx = np.random.choice(len(X_test_np), size=2000, replace=False)\n",
    "X_shap = X_test_fe.iloc[shap_sample_idx].reset_index(drop=True)\n",
    "X_shap_np = X_shap.values\n",
    "\n",
    "print('Computing SHAP values (TreeExplainer)...')\n",
    "explainer = shap.TreeExplainer(base_gbm)\n",
    "shap_values = explainer.shap_values(X_shap_np)\n",
    "\n",
    "print(f'SHAP values computed. Shape: {shap_values.shape}')\n",
    "print(f'Base value (expected prediction): {explainer.expected_value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (beeswarm)\n",
    "# Each dot = one loan. X-axis = SHAP value. Color = feature value (red=high, blue=low)\n",
    "plt.figure(figsize=(10, 7))\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_shap,\n",
    "    max_display=15,\n",
    "    show=False,\n",
    ")\n",
    "plt.title('SHAP Feature Impact — Gradient Boosting (Top 15 Features)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'shap_summary.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved to reports/figures/shap_summary.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Mean Absolute Feature Importance Bar Chart\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "fig = plot_feature_importance_bar(\n",
    "    feature_names=feature_names,\n",
    "    mean_abs_shap=mean_abs_shap,\n",
    "    top_n=15,\n",
    "    save_path=os.path.join(FIG_DIR, 'shap_importance_bar.png')\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Print top 10 features\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'mean_abs_shap': mean_abs_shap})\n",
    "importance_df = importance_df.sort_values('mean_abs_shap', ascending=False).head(10)\n",
    "print('Top 10 features by mean absolute SHAP value:')\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Waterfall Plot — explain a specific high-risk loan\n",
    "# Find the loan with the highest predicted default probability in our SHAP sample\n",
    "shap_sample_probs = gb_c_probs[shap_sample_idx]\n",
    "high_risk_idx = int(np.argmax(shap_sample_probs))\n",
    "\n",
    "print(f'High-risk loan (sample index {high_risk_idx}):')\n",
    "print(f'  Predicted PD:    {shap_sample_probs[high_risk_idx]:.1%}')\n",
    "print(f'  Actual outcome:  {\"Default\" if y_test[shap_sample_idx[high_risk_idx]] == 1 else \"Performing\"}')\n",
    "print(f'  Credit score:    {X_shap[\"credit_score\"].iloc[high_risk_idx]}')\n",
    "print(f'  DTI:             {X_shap[\"debt_to_income\"].iloc[high_risk_idx]:.2f}')\n",
    "print(f'  Utilization:     {X_shap[\"credit_utilization\"].iloc[high_risk_idx]:.2f}')\n",
    "\n",
    "# Full SHAP explanation\n",
    "explanation = explainer(X_shap_np)\n",
    "\n",
    "shap.waterfall_plot(explanation[high_risk_idx], show=False, max_display=12)\n",
    "plt.title(f'SHAP Waterfall — High-Risk Loan (PD={shap_sample_probs[high_risk_idx]:.1%})',\n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'shap_waterfall.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Dependence Plot — Credit Score\n",
    "fig = plot_shap_dependence(\n",
    "    shap_values=shap_values,\n",
    "    X=X_shap,\n",
    "    feature='credit_score',\n",
    "    interaction_feature='credit_utilization',\n",
    "    save_path=os.path.join(FIG_DIR, 'shap_dependence_credit_score.png'),\n",
    "    figsize=(9, 6)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Dependence Plot — Credit Utilization\n",
    "fig = plot_shap_dependence(\n",
    "    shap_values=shap_values,\n",
    "    X=X_shap,\n",
    "    feature='credit_utilization',\n",
    "    interaction_feature='num_delinquencies',\n",
    "    save_path=os.path.join(FIG_DIR, 'shap_dependence_utilization.png'),\n",
    "    figsize=(9, 6)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Interpretability Findings\n",
    "\n",
    "The SHAP analysis reveals the drivers of the model's predictions:\n",
    "\n",
    "1. **Credit score** is the dominant feature — low scores strongly push predictions toward default (large positive SHAP values). The dependence plot shows a roughly monotone negative relationship, with the steepest effect between 580–700 (Fair and lower-Good range).\n",
    "\n",
    "2. **Credit utilization** is the second most important variable. High utilization (>60%) substantially increases predicted default probability. The interaction with delinquency count is visible in the dependence plot — the effect is amplified for borrowers who also have past delinquencies.\n",
    "\n",
    "3. **Interaction terms** like `credit_score_x_utilization` and `payment_to_income` appear in the top features, confirming that the engineered features added genuine signal.\n",
    "\n",
    "4. **The waterfall plot** for a specific high-risk loan provides the raw material for an adverse action notice: \"Your application was declined primarily because of (1) low credit score, (2) high credit utilization, and (3) recent delinquency history.\"\n",
    "\n",
    "5. **Long employment history and high income** appear as negative (risk-reducing) SHAP contributors, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9 — Conclusions & Model Card\n",
    "\n",
    "### Model Card: Loan Default Risk Model v1.0\n",
    "\n",
    "---\n",
    "\n",
    "**Model type:** Gradient Boosting Classifier with Platt scaling calibration  \n",
    "**Training data:** 40,000 loans (synthetic, calibrated to US near-prime lending distributions)  \n",
    "**Evaluation data:** 10,000 held-out loans (stratified split, never used in training or calibration)  \n",
    "**Intended use:** Binary classification of consumer loan applications into default / non-default risk; probability output used in threshold-based approval decisions  \n",
    "\n",
    "---\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "| Metric | Logistic Regression | Gradient Boosting (calibrated) |\n",
    "|---|---|---|\n",
    "| ROC-AUC | ~0.76 | ~0.83 |\n",
    "| KS Statistic | ~0.41 | ~0.52 |\n",
    "| Gini Coefficient | ~0.52 | ~0.66 |\n",
    "| Brier Score | ~0.135 | ~0.122 |\n",
    "| Average Precision | ~0.52 | ~0.63 |\n",
    "\n",
    "---\n",
    "\n",
    "### Recommended Deployment\n",
    "\n",
    "- **Model:** Calibrated Gradient Boosting (`models/gradient_boosting_calibrated.pkl`)\n",
    "- **Approval threshold:** ~0.22 predicted default probability (profit-maximising on test set)\n",
    "- **Expected approval rate:** ~70% of applicants\n",
    "- **Expected default reduction:** from 20% (population) to ~11% in approved pool\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations & Assumptions\n",
    "\n",
    "| Limitation | Implication |\n",
    "|---|---|\n",
    "| Synthetic training data | Model may not generalise to real portfolios without recalibration on live data |\n",
    "| Static feature distributions | No temporal drift modelling; requires Population Stability Index (PSI) monitoring in production |\n",
    "| 10% flat recovery rate | Actual recovery varies by loan size, collateral, collection agency; use LGD model for precision |\n",
    "| No vintage analysis | Default rates vary by economic cycle; model should be stress-tested |\n",
    "| Single threshold | Advanced implementations use risk-tiered pricing (interest rate = f(PD)) rather than binary approve/decline |\n",
    "\n",
    "---\n",
    "\n",
    "### Recommended Next Steps\n",
    "\n",
    "1. **Drift monitoring:** Implement Population Stability Index (PSI) on input features and Performance Stability Index (PSI) on model output scores — alert if PSI > 0.10\n",
    "\n",
    "2. **Champion/Challenger:** Deploy this model as champion; A/B test a new challenger (XGBoost, LightGBM, or a neural network) with 10–20% traffic split\n",
    "\n",
    "3. **Vintage analysis:** Track cohort-level default rates vs. model-predicted PDs over 12- and 24-month windows to validate through-the-cycle calibration\n",
    "\n",
    "4. **Loss Given Default (LGD) model:** Pair this PD model with a separate LGD model for more accurate expected loss estimates, enabling risk-based pricing\n",
    "\n",
    "5. **Fairness analysis:** Run disparate impact analysis across protected characteristics (age, gender, race via proxy) using demographic data; ensure adverse action rate differentials are within regulatory guidance\n",
    "\n",
    "6. **Scorecard conversion:** For regulatory transparency, consider converting the GBM into a linear scorecard using Weight of Evidence (WoE) encoding, which preserves most predictive power while producing fully interpretable integer scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary printout\n",
    "print('='*60)\n",
    "print(' LOAN DEFAULT RISK MODELING — FINAL SUMMARY')\n",
    "print('='*60)\n",
    "print(f' Dataset:         50,000 synthetic consumer loans')\n",
    "print(f' Default rate:    {df[\"default\"].mean():.1%}')\n",
    "print(f' Features:        {len(feature_names)} (after engineering)')\n",
    "print(f'')\n",
    "print(f' Best model:      Gradient Boosting (Calibrated)')\n",
    "\n",
    "best_gb_auc, _, _, _ = compute_roc_auc(y_test, gb_c_probs)\n",
    "best_gb_ks, _ = compute_ks_statistic(y_test, gb_c_probs)\n",
    "best_gb_gini = compute_gini_coefficient(best_gb_auc)\n",
    "best_gb_brier = compute_brier_score(y_test, gb_c_probs)\n",
    "\n",
    "print(f'   ROC-AUC:       {best_gb_auc:.4f}')\n",
    "print(f'   KS-Statistic:  {best_gb_ks:.4f}')\n",
    "print(f'   Gini:          {best_gb_gini:.4f}')\n",
    "print(f'   Brier Score:   {best_gb_brier:.4f}')\n",
    "print(f'')\n",
    "print(f' Optimal threshold: {opt_threshold:.2f} (profit-maximising)')\n",
    "print(f'   Approval rate:   {portfolio[\"approval_rate\"]:.1%}')\n",
    "print(f'   Portfolio profit: ${portfolio[\"expected_total_profit\"]:,.0f}')\n",
    "print(f'   Return on book:  {portfolio[\"return_on_portfolio\"]:.2%}')\n",
    "print(f'')\n",
    "print(f' Saved artefacts:')\n",
    "print(f'   models/logistic_regression.pkl')\n",
    "print(f'   models/gradient_boosting.pkl')\n",
    "print(f'   models/gradient_boosting_calibrated.pkl')\n",
    "print(f'   data/raw/loans_raw.csv')\n",
    "print(f'   data/processed/loans_processed.csv')\n",
    "print(f'   reports/figures/*.png  ({len(os.listdir(FIG_DIR))} figures)')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
